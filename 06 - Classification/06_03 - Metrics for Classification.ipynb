{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5552345e",
   "metadata": {},
   "source": [
    "# 06.03 - Metrics for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cd6cb",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix, also known as an error matrix, is a common tool in machine learning, specifically in statistical classification. This matrix is a table that allows for the visualization of an algorithm's performanceâ€”usually a supervised learning algorithm. In unsupervised learning, this is typically referred to as a matching matrix.\n",
    "\n",
    "The confusion matrix is a 2x2 table that reports four values: true positives, false negatives, false positives, and true negatives. This gives a more detailed analysis than simply looking at the proportion of correct classifications, or accuracy. Accuracy can be misleading if the data set is unbalanced, meaning the numbers of observations in different classes vary greatly.\n",
    "\n",
    "For instance, if a data set had 95 cancer samples and 5 non-cancer samples, a classifier might classify all observations as cancer. While the overall accuracy would be 95%, the classifier would have a 100% recognition rate (sensitivity) for the cancer class but a 0% recognition rate for the non-cancer class. The F1 score can also be unreliable in such cases, potentially yielding over 97.4%, whereas informedness removes bias and yields 0 as the probability of an informed decision for any form of guessing.\n",
    "\n",
    "According to researchers Davide Chicco and Giuseppe Jurman, the Matthews correlation coefficient (MCC) is the most informative metric to evaluate a confusion matrix. However, many other metrics can be included in a confusion matrix, each with their own significance and use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26567559",
   "metadata": {},
   "source": [
    "![](./confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590cc3a8",
   "metadata": {},
   "source": [
    "## Common Metrics for Classification\n",
    "\n",
    "### $ \\text{Total Population} = P + N $\n",
    "### $ \\text{Prevalence} = \\frac{P}{P+N} $\n",
    "### $ \\text{Accuracy (ACC)} = \\frac{TP + TN}{P + N} $\n",
    "---\n",
    "### $ \\text{True Positive Rate (TPR)} = \\frac{TP}{P} = 1 - \\text{FNR} $\n",
    "### $ \\text{True Negative Rate (TNR)} = \\frac{TN}{N} = 1 - \\text{FPR} $\n",
    "### $ \\text{False Positive Rate (TPR)} = \\frac{FP}{N} = 1 - \\text{TNR} $\n",
    "### $ \\text{False Negative Rate (TNR)} = \\frac{FN}{P} = 1 - \\text{TPR} $\n",
    "---\n",
    "### $ \\text{Balanced Accuracy (BA)} = \\frac{\\text{TPR}+\\text{TNR}}{2} $\n",
    "### $ F_1\\text{ Score} = \\frac{2 * \\text{TP}}{(2 * \\text{TP}) + \\text{FP} + \\text {FN}} $\n",
    "---\n",
    "### $ \\text{Positive Predictive Value (PPV)} = \\frac{TP}{PP} = 1 - \\text{FDR} $\n",
    "### $ \\text{Negative Predictive Value (NPV)} = \\frac{TN}{PN} = 1 - \\text{FOR} $\n",
    "### $ \\text{False Discovery Rate (FDR)} = \\frac{FP}{PP} = 1 - \\text{PPV} $\n",
    "### $ \\text{False Omission Rate (FOR)} = \\frac{FN}{PN} = 1 - \\text{NPV} $\n",
    "---\n",
    "### $ \\text{Matthews Correlation Coefficient (MCC)} = \\sqrt{\\text{TPR} * \\text{TNR} * \\text{PPV} * \\text{NPV}} - \\sqrt{\\text{FNR} * \\text{FPR} * \\text{FOR} * \\text{FDR}} $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
