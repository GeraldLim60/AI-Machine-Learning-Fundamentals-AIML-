{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5840597e",
   "metadata": {},
   "source": [
    "# 07.04 - Regularisation (Elastic Net Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dabd45",
   "metadata": {},
   "source": [
    "Elastic Net is a hybrid regularization technique that combines the strengths of both Ridge and Lasso regression. It incorporates their penalties into the loss function to optimize the model's performance.\n",
    "\n",
    "The equation to minimize in Elastic Net is given as:\n",
    "\n",
    "### $$ \\text{minimize:}\\; RSS + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\rho\\sum_{j=1}^p |\\beta_j| + \\alpha(1-\\rho)\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "This equation is comprised of three main parts:\n",
    "\n",
    "- The Residual Sum of Squares (RSS) which measures the amount of variance in the data that the model did not account for.\n",
    "- The Ridge Penalty, which is used to prevent overfitting by minimizing the magnitude of the coefficients.\n",
    "- The Lasso Penalty, which is used to induce sparsity by setting some coefficients to zero.\n",
    "\n",
    "The balance between the Ridge and Lasso penalties in Elastic Net is controlled by the ρ parameter. This parameter, which lies between zero and one, determines the ratio of the Lasso penalty to the Ridge penalty.\n",
    "\n",
    "The `ElasticNet` function in sklearn has two parameters:\n",
    "\n",
    "- `alpha`: This is the regularization strength, which dictates the amount of shrinkage. Higher values of alpha result in greater regularization which can help prevent overfitting.\n",
    "- `l1_ratio`: This is equivalent to ρ and it determines the mix of Lasso vs Ridge penalties. An `l1_ratio` of 0 is equivalent to Ridge regression, while an `l1_ratio` of 1 is equivalent to Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4f1615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>ph</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>red_wine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    ph  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  red_wine  \n",
       "0      9.4        5         1  \n",
       "1      9.8        5         1  \n",
       "2      9.8        5         1  \n",
       "3      9.8        6         1  \n",
       "4      9.4        5         1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas library which provides data structures and data analysis tools\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dataset from the 'wine.csv' file using the read_csv function provided by pandas.\n",
    "# The file is assumed to be in the same directory as this script ('./' denotes current directory).\n",
    "# The result is stored in the DataFrame `df`.\n",
    "df: pd.DataFrame = pd.read_csv('./wine.csv')\n",
    "\n",
    "# Replace spaces in column names and convert all columns to lowercase:\n",
    "# This is done by iterating over each column name in df.columns,\n",
    "# replacing spaces with underscores using the replace function,\n",
    "# and converting to lowercase using the lower function.\n",
    "# The resulting list of modified column names is then assigned back to df.columns.\n",
    "df.columns = [x.lower().replace(' ','_') for x in df.columns]\n",
    "\n",
    "# Display the first 5 rows of the DataFrame using the head function.\n",
    "# This is useful for quickly testing if your object has the right type of data in it.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8be7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are defining our target or dependent variable that our model will predict.\n",
    "# In machine learning, the target variable is the variable that the algorithm aims to predict or forecast.\n",
    "# In our case, we are predicting the 'quality' of the wine based on other features in our dataset.\n",
    "# We store the column name of our target variable as a string in the variable 'target'.\n",
    "target: str = 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55878030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from typing import List\n",
    "\n",
    "# We first want to select all columns that are not our target column.\n",
    "# We create a list comprehension that iterates over all column names in our DataFrame,\n",
    "# and only keeps the column name if it is not equal to our target.\n",
    "# The resulting list 'nc' (non-class columns) contains all column names that are not our target.\n",
    "nc: List[str] = [x for x in df.columns if x != target]\n",
    "\n",
    "# Next, we want to normalize our data. Normalization is the process of scaling individual samples to have unit norm.\n",
    "# This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel\n",
    "# to quantify the similarity of any pair of samples.\n",
    "# Here, we subtract the mean and divide by the standard deviation for each non-target column in our DataFrame.\n",
    "# This scales all predictor variables to have a mean of 0 and a standard deviation of 1.\n",
    "# This is important for many machine learning algorithms, as they can perform poorly if the input variables are not on similar scales.\n",
    "# By using the mean() and std() functions, we are able to easily calculate the mean and standard deviation for each column.\n",
    "# Note: We could have also used sklearn's StandardScaler for this step.\n",
    "df[nc] = (df[nc] - df[nc].mean()) / df[nc].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc68d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define `X` as the values in the non-class columns of the DataFrame\n",
    "# `.values` is used to return the numpy representation of the DataFrame.\n",
    "X = df[nc].values\n",
    "\n",
    "# Define `y` as the values in the target column of the DataFrame\n",
    "y = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d7053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PolynomialFeatures class from sklearn.preprocessing\n",
    "# PolynomialFeatures is a class in the sklearn.preprocessing package that generates polynomial and interaction features.\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Initialise a PolynomialFeatures object\n",
    "# The parameters for PolynomialFeatures are:\n",
    "# - degree: The degree of the polynomial features. Default = 2.\n",
    "# - interaction_only: If true, only interaction features are produced: features that are products of at most degree distinct input features (so not x[1]**2, x[0]*x[2]**3, etc.).\n",
    "# - include_bias: If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).\n",
    "pf = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "# Fit the PolynomialFeatures object to the `X` data\n",
    "# The fit method computes the mean and std to be used for later scaling.\n",
    "pf = pf.fit(X)\n",
    "\n",
    "# Transform the `X` data to its polynomial features representation.\n",
    "# This method applies the fitted transformation to the `X` data, returning the transformed data.\n",
    "Xoverfit = pf.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d56686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function from sklearn.model_selection\n",
    "# train_test_split is a function in Sklearn model selection for splitting data arrays into two subsets: for training data and for testing data.\n",
    "# With this function, you don't need to divide the dataset manually.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Now we use the train_test_split function to split our dataset into training and testing sets.\n",
    "# X is the array of predictor variables, and y is the array of response variables.\n",
    "# The test_size parameter specifies the proportion of the dataset to include in the test split (30% in this case).\n",
    "# The random_state parameter is a seed for the random number generator; using a fixed number ensures that the output will be the same each time this code is run.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "# We perform the same operation on the overfit data, Xoverfit.\n",
    "# Here, we do not have a corresponding 'yo_train' or 'yo_test' because the target values remain the same; only the predictor variables have been transformed.\n",
    "Xo_train, Xo_test = train_test_split(Xoverfit, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccad7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ElasticNet from sklearn.linear_model\n",
    "# ElasticNet is a linear regression model trained with both l1 and l2 -norm regularization of the coefficients.\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Define an ElasticNet regression model with specified `alpha` and `l1_ratio` parameters.\n",
    "# `alpha` is the constant that multiplies the penalty terms and `l1_ratio` is the ElasticNet mixing parameter with `0 <= l1_ratio <= 1`.\n",
    "# `l1_ratio` of 0 corresponds to L2 penalty, 1 to L1.\n",
    "enet_reg = ElasticNet(alpha = 1, l1_ratio = 0.05)\n",
    "\n",
    "# Fit the ElasticNet model on the training data.\n",
    "enet_reg.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to predict the target variable on the test data.\n",
    "y_pred = enet_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32494f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7685849129418407\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import mean_squared_error function from sklearn.metrics\n",
    "# mean_squared_error is a risk metric corresponding to the expected value of the squared (quadratic) error or loss.\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# Calculate the root mean squared error (RMSE) which is the square root of the mean squared error.\n",
    "rmse = np.sqrt(mse(y_test, y_pred))\n",
    "\n",
    "# Print the calculated RMSE.\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1097ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7604368453549241\n"
     ]
    }
   ],
   "source": [
    "# Define and fit a new ElasticNet model on the overfit training data.\n",
    "enet_reg = ElasticNet(alpha = 1, l1_ratio = 0.05)\n",
    "enet_reg.fit(Xo_train, y_train)\n",
    "\n",
    "# Predict the target variable on the overfit test data.\n",
    "yo_pred = enet_reg.predict(Xo_test)\n",
    "\n",
    "# Calculate the RMSE for the overfit data.\n",
    "rmse = np.sqrt(mse(y_test, yo_pred))\n",
    "\n",
    "# Print the calculated RMSE for the overfit data.\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7a4af02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared for Elastic Net Regression: 0.185375795883868\n",
      "Adjusted R-squared for Elastic Net Regression Overfit: 0.1744265161404044\n"
     ]
    }
   ],
   "source": [
    "# Import r2_score from sklearn.metrics\n",
    "# r2_score computes the coefficient of determination, a measure of how well observed outcomes are replicated by the model.\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate the R-squared score for both the original and overfit data.\n",
    "enet_r2 = r2_score(y_test, y_pred)\n",
    "overfit_r2 = r2_score(y_test, yo_pred)\n",
    "\n",
    "# Calculate the number of samples in the test set.\n",
    "n = len(y_test)\n",
    "\n",
    "# Calculate the number of features in the original and overfit training sets.\n",
    "p = X_train.shape[1]\n",
    "po = Xo_train.shape[1]\n",
    "\n",
    "# Calculate the adjusted R-squared for both the original and overfit data.\n",
    "# The adjusted R-squared compensates for the addition of variables and only increases if the new variable improves the model more than would be expected by chance.\n",
    "enet_adj_r2 = 1 - (1 - enet_r2) * ((n - 1) / (n - p - 1))\n",
    "overfit_adj_r2 = 1 - (1 - overfit_r2) * ((n - 1) / (n - po - 1))\n",
    "\n",
    "# Print the adjusted R-squared for both the original and overfit data.\n",
    "print(f'Adjusted R-squared for Elastic Net Regression: {enet_adj_r2}')\n",
    "print(f'Adjusted R-squared for Elastic Net Regression Overfit: {overfit_adj_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e62b0c",
   "metadata": {},
   "source": [
    "The Adjusted R-squared value is a statistical measure that provides a gauge of the goodness-of-fit of a regression model. Unlike the R-squared value, the Adjusted R-squared takes into account the number of predictors in the model, and adjusts accordingly. It increases only if the new variable improves the model more than what would be predicted by chance. It incorporates the degrees of freedom, and can handle the issue of overfitting when too many predictors are included in the model.\n",
    "\n",
    "In the given output, we have two Adjusted R-squared values:\n",
    "\n",
    "1. For the Elastic Net Regression model, the Adjusted R-squared is approximately 0.185. This means that around 18.5% of the variability in the dependent variable (wine quality) can be explained by the model. Considering the amount of predictors used in the model, this is the proportion of the total variation that's captured.\n",
    "2. For the Overfit Elastic Net Regression model, the Adjusted R-squared is approximately 0.174. Despite the model being overfitted (with more features generated using PolynomialFeatures), the model explains about 17.4% of the variability in the dependent variable. This lower score, compared to the non-overfit model, indicates that the additional predictors in the overfit model may not contribute significantly to the model's predictive power, and may instead be causing the model to overfit the training data.\n",
    "\n",
    "Both values are relatively low, suggesting that there is a lot of variability in the wine quality that these models are not capturing. This may suggest the need for further refinement of the models, potentially through the inclusion of more relevant predictors, better feature engineering, or the use of a different modeling approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
