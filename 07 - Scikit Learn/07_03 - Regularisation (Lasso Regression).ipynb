{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e651f6a9",
   "metadata": {},
   "source": [
    "# 07.03 - Regularisation (Lasso Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ca573",
   "metadata": {},
   "source": [
    "## Defining and Visualizing the Lasso Regression\n",
    "\n",
    "In this lab, we're going to look at the Lasso Regression, which is a different approach from the Ridge Regression we've looked at previously. With Lasso Regression, we will see a difference in how the coefficients change.\n",
    "\n",
    "Lasso Regression, much like Ridge Regression, is a type of linear regression that uses shrinkage. Shrinkage is where the data values are shrunk towards a central point, like the mean. However, the key difference in Lasso Regression is the way it penalizes the coefficients of the predictors.\n",
    "\n",
    "In Lasso Regression, the loss function is altered by adding the absolute value of the magnitude of the coefficient as a penalty term to the Residual Sum of Squares (RSS). The formula for this is as follows:\n",
    "\n",
    "### $$ \\text{minimize:}\\; RSS + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "Here, $|\\beta_j|$ represents the absolute value of the $\\beta$ coefficient for the variable $x_j$.\n",
    "\n",
    "Meanwhile, $\\alpha$ is a parameter we need to choose that decides the strength of the penalty term. As with Ridge Regression, increasing the value of Œ± will shrink the coefficients more, potentially setting some coefficients to zero if Œ± is large enough. This is a form of feature selection, where the model selects which features to include by shrinking the coefficients of less important features to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eec54d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>ph</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>red_wine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    ph  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  red_wine  \n",
       "0      9.4        5         1  \n",
       "1      9.8        5         1  \n",
       "2      9.8        5         1  \n",
       "3      9.8        6         1  \n",
       "4      9.4        5         1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pandas library as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dataset from the file 'wine.csv' into a pandas DataFrame\n",
    "# The file path './wine.csv' indicates that the file is in the same directory as the current script\n",
    "# The DataFrame is stored in the variable df\n",
    "df = pd.read_csv('./wine.csv')\n",
    "\n",
    "# The column names of the DataFrame are updated\n",
    "# For each column name, the spaces are replaced with underscores and the text is converted to lowercase\n",
    "# This is done using a list comprehension, where x is the name of each column in the DataFrame\n",
    "# The replace() function replaces spaces with underscores, and the lower() function converts the text to lowercase\n",
    "df.columns = [x.lower().replace(' ','_') for x in df.columns]\n",
    "\n",
    "# The head() function is called on the DataFrame to display the first 5 rows\n",
    "# This is often done for a quick overview of the data after it is loaded\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91b6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will identify the target variable, which is the variable we aim to predict.\n",
    "# In machine learning, the target variable is also often referred to as the dependent variable.\n",
    "# In this case, our target variable is 'quality'.\n",
    "# We store the name of the target variable in the string variable `target`.\n",
    "# We can use this `target` variable later in our code to refer to the column in the DataFrame that contains the target values.\n",
    "target: str = 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0d08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# First, we want to select all the columns that are not the target variable.\n",
    "# To do this, we create a list comprehension that iterates over the column names in the wine DataFrame.\n",
    "# If the column name is not equal to the target variable, it is added to the list.\n",
    "# The list of non-target column names is stored in the variable `nc`.\n",
    "nc: List[str] = [x for x in df.columns if x != target]\n",
    "\n",
    "# We then want to normalize our predictor variables.\n",
    "# Normalizing means adjusting the values measured on different scales to a common scale.\n",
    "# In this case, we are normalizing so that all predictor variables have a mean of 0 and a standard deviation of 1.\n",
    "# This is done by subtracting the mean and dividing by the standard deviation for each predictor variable.\n",
    "# We apply this normalization to all columns in `nc`, which contains all non-target variables.\n",
    "# Note: We could also use sklearn's StandardScaler for this task.\n",
    "df[nc] = (df[nc] - df[nc].mean()) / df[nc].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6777d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our predictor matrix X as the values of all non-target columns in the DataFrame\n",
    "# The .values attribute of a DataFrame returns a Numpy array of the DataFrame values\n",
    "X = df[nc].values\n",
    "\n",
    "# Similarly, we define our target matrix Y as the values of the target column in the DataFrame\n",
    "y = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b9d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PolynomialFeatures class from sklearn.preprocessing\n",
    "# PolynomialFeatures is used to generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# We initialize a PolynomialFeatures object with degree=2\n",
    "# The degree parameter determines the maximum degree of the polynomial transformations\n",
    "# interaction_only=True means that only interaction features are produced: features that are products of at most degree distinct input features\n",
    "# include_bias=False means that a bias column (a column of ones) is not added\n",
    "pf = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "# We fit the PolynomialFeatures object to our predictor matrix X\n",
    "# This calculates the number of output features, which is necessary before we can use the transform method\n",
    "pf = pf.fit(X)\n",
    "\n",
    "# We transform our predictor matrix X using the fitted PolynomialFeatures object\n",
    "# This applies the polynomial feature transformations to our data, creating a new overfit predictor matrix Xoverfit\n",
    "Xoverfit = pf.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b745c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function from sklearn.model_selection\n",
    "# This function is used to split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The train_test_split function is called with four arguments:\n",
    "# - X: the predictors matrix\n",
    "# - y: the target vector\n",
    "# - test_size: the proportion of the dataset to include in the test split\n",
    "# - random_state: a seed used by the random number generator to ensure the same random split each time the script is run\n",
    "# The function returns four outputs:\n",
    "# - X_train: the predictors matrix for the training set\n",
    "# - X_test: the predictors matrix for the test set\n",
    "# - y_train: the target vector for the training set\n",
    "# - y_test: the target vector for the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "# The train_test_split function is called again to split the overfit predictors matrix into training and test sets\n",
    "# The outputs are stored in Xo_train and Xo_test, which are the overfit predictors matrices for the training and test sets, respectively\n",
    "Xo_train, Xo_test = train_test_split(Xoverfit, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b386248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lasso class from sklearn.linear_model\n",
    "# Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent.\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha = 0.15\n",
    "\n",
    "# Initialize a Lasso object\n",
    "lasso_reg = Lasso(alpha = alpha)\n",
    "\n",
    "# Fit the Lasso model to the training data\n",
    "# The .fit() method calculates the optimal values of the weights ùëè‚ÇÄ, ùëè‚ÇÅ, ‚Ä¶, ùëè·µ£, using the existing input and output (x and y) as the arguments\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "# We use the .predict() method with X_test as the argument\n",
    "y_pred = lasso_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "924e9b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7616484497594653\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import the mean_squared_error function from sklearn.metrics\n",
    "# Mean Squared Error (MSE) is a measure of how close a fitted line is to actual data points\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# Calculate the root mean squared error (RMSE) which is the square root of MSE\n",
    "rmse = np.sqrt(mse(y_test, y_pred))\n",
    "\n",
    "# Print the RMSE\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "937dbbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7616484497594653\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Lasso object\n",
    "lasso_reg = Lasso(alpha = alpha)\n",
    "\n",
    "# Fit the Lasso model to the overfit training data\n",
    "lasso_reg.fit(Xo_train, y_train)\n",
    "\n",
    "# Predict the response for the overfit test dataset\n",
    "yo_pred = lasso_reg.predict(Xo_test)\n",
    "\n",
    "# Calculate the root mean squared error (RMSE) for the overfit data\n",
    "rmse = np.sqrt(mse(y_test, yo_pred))\n",
    "\n",
    "# Print the RMSE\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09dc851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared for Lasso Regression: 0.20001337822754262\n",
      "Adjusted R-squared for Lasso Regression Overfit: 0.17179364704796918\n"
     ]
    }
   ],
   "source": [
    "# Import the r2_score function from sklearn.metrics\n",
    "# R-squared is a statistical measure that represents the goodness of fit of a regression model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate the R-squared score for the Lasso Regression model\n",
    "lasso_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the R-squared score for the Lasso Regression model on the overfit data\n",
    "overfit_r2 = r2_score(y_test, yo_pred)\n",
    "\n",
    "# Calculate the number of samples (n) in the test set\n",
    "n = len(y_test)\n",
    "\n",
    "# Calculate the number of predictors (p) in the training set\n",
    "p = X_train.shape[1]\n",
    "\n",
    "# Calculate the number of predictors (po) in the overfit training set\n",
    "po = Xo_train.shape[1]\n",
    "\n",
    "# Calculate the adjusted R-squared for the Lasso Regression model\n",
    "# Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model\n",
    "lasso_adj_r2 = 1 - (1 - lasso_r2) * ((n - 1) / (n - p - 1))\n",
    "\n",
    "# Calculate the adjusted R-squared for the Lasso Regression model on the overfit data\n",
    "overfit_adj_r2 = 1 - (1 - overfit_r2) * ((n - 1) / (n - po - 1))\n",
    "\n",
    "# Print the adjusted R-squared for the Lasso Regression model\n",
    "print(f'Adjusted R-squared for Lasso Regression: {lasso_adj_r2}')\n",
    "\n",
    "# Print the adjusted R-squared for the Lasso Regression model on the overfit data\n",
    "print(f'Adjusted R-squared for Lasso Regression Overfit: {overfit_adj_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49cd14f",
   "metadata": {},
   "source": [
    "The adjusted R-squared for the Lasso Regression is approximately 0.20, while the adjusted R-squared for the Lasso Regression for the overfit data is approximately 0.17. These values represent the proportion of the total variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "The R-squared statistic provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. However, there is one drawback to the R-squared statistic: it will either stay the same or increase with the addition of more variables, even if those variables are only weakly associated with the response.\n",
    "\n",
    "This is where adjusted R-squared comes in. The adjusted R-squared compensates for the addition of variables and only increases if the new variable improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.\n",
    "\n",
    "In this case, the lower adjusted R-squared for the overfit data suggests that some of the additional predictors in the model do not significantly improve its performance, and may in fact be detracting from the quality of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
