{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04.02 - Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal is to create a mathematical model that accurately describes the relationship between the independent and dependent variables.\n",
    "\n",
    "## Load the diamonds data\n",
    "\n",
    "In this lab, we'll be using a dataset on diamond prices to construct a simple linear regression, and then extend this to multiple linear regression. The diamond dataset contains information such as carat, cut, color, and price, making it an excellent candidate for understanding the concepts of multiple linear regression.\n",
    "\n",
    "To get started, we'll need to load the data. We'll do this using pandas, a powerful data manipulation and analysis library in Python. To load the data, we'll use the `pandas.read_csv()` function, which allows us to read in a CSV and convert it directly into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the pandas library, which provides data structures and data analysis tools for Python\n",
    "import pandas as pd\n",
    "\n",
    "# Specifies the file path to the diamonds CSV file.\n",
    "# The CSV file contains data on diamonds that we will be using for our analysis.\n",
    "diamonds_csv = './diamonds.csv'\n",
    "\n",
    "# Using the read_csv function from the pandas library to read the CSV file specified by 'diamonds_csv'\n",
    "# The result is a DataFrame, which is a two-dimensional labeled data structure with columns of potentially different types.\n",
    "# DataFrames are generally the most commonly used pandas object.\n",
    "diamonds = pd.read_csv(diamonds_csv)\n",
    "\n",
    "# The head() function returns the first 5 rows of the DataFrame.\n",
    "# This is often used for quickly testing if your object (in this case, 'diamonds') has the right type of data in it.\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the dataset represent the following features of each diamond:\n",
    "\n",
    "- `carat`: This column represents the carat weight of each diamond. Carat is a unit of weight for diamonds where one carat equals to 200 milligrams. The weight of a diamond has a significant impact on its price, as larger diamonds are rarer and more desirable.\n",
    "- `depth`, `table`, `x`, `y`, `z`: These columns represent the physical measurements of each diamond.\n",
    "    - `depth` is the height of a diamond, measured from the culet (bottom) to the table (top).\n",
    "    - `table` refers to the width of the diamond's top facet in relation to the width of the diamond overall.\n",
    "    - `x`, `y`, and `z` represent the dimensions of the diamond in millimeters. `x` is the length, `y` is the width, and `z` is the depth.\n",
    "- `price`: This column represents the price of each diamond. This is the dependent variable that we're interested in predicting with our multiple linear regression model.\n",
    "\n",
    "## From Simple Linear Regression (SLR) to Multiple Linear Regression (MLR)\n",
    "\n",
    "While simple linear regression utilizes a single predictor to estimate a continuous target, multiple linear regression expands this concept by building a model with multiple predictor variables. In practice, you will find multiple linear regression being used more frequently than its simpler counterpart. These variables are typically represented as columns in a matrix, which is often a pandas DataFrame in the context of Python programming.\n",
    "\n",
    "Let's take a moment to consider real-world scenarios where having multiple predictors would be beneficial. A common example would be predicting tomorrow's weather, which could depend on multiple factors such as atmospheric pressure and temperature, rather than just one of them. Hence, a multiple linear regression model that takes both these predictors into account would likely produce more accurate forecasts.\n",
    "\n",
    "However, it's important to note that multiple linear regression can also be detrimental in certain cases. This is particularly true if the predictors included in the model aren't relevant to the outcome we're trying to predict. Including predictors that don't have a real impact on the dependent variable can introduce unnecessary complexity into our model, potentially making our predictions less accurate and our model more difficult to interpret. We'll delve deeper into this topic later.\n",
    "\n",
    "## Assumptions of Multiple Linear Regression\n",
    "\n",
    "Just like Simple Linear Regression (SLR), Multiple Linear Regression (MLR) comes with its own set of assumptions. Fortunately, these assumptions closely mirror those of SLR and can be remembered using the mnemonic LINEI. Let's delve into each one of these assumptions in more detail:\n",
    "\n",
    "1. **Linearity:** This assumes that there is a linear relationship between the dependent variable, $Y$, and each of the independent variables, $X_i$. In other words, changes in $X_i$ will result in proportional changes in $Y$.\n",
    "2. **Independence:** The residuals (the differences between the observed and predicted values of $Y$), denoted as $\\varepsilon_i$ and $\\varepsilon_j$, should be independent of each other for all $i \\ne j$. This means that the residual from any one prediction does not influence the residual from any other prediction.\n",
    "3. **Normality:** This assumes that the residuals follow a Normal distribution centered around 0. In other words, the distribution of residuals should ideally form a bell curve where the mean and median coincide at 0.\n",
    "4. **Equality of Variances:** This assumption, also known as homoscedasticity, posits that the distribution of residuals should show a roughly consistent spread across all levels of the independent variables, $X_i$. There should be no discernable pattern or relationship between the predictors and the residuals.\n",
    "5. **Independence of Predictors:** The independent variables, $X_i$ and $X_j$, should be independent of each other for all $i \\ne j$. This means that there is no perfect multicollinearity, or exact linear relationship, between two or more of the independent variables.\n",
    "\n",
    "Each of these assumptions plays a critical role in ensuring the validity and reliability of a multiple linear regression model. Violations of these assumptions may lead to erroneous or misleading results, underscoring the importance of checking these assumptions before making inferences from a MLR model.\n",
    "\n",
    "## Fitting a Multiple Linear Regression Model\n",
    "\n",
    "In multiple linear regression, the computation of the beta coefficients is best done using linear algebra. The formula for multiple linear regression expands the simple linear regression equation to incorporate multiple predictors.\n",
    "\n",
    "Instead of using a single predictor, we now have a matrix, `X`, of predictors ranging from `x1` to `xi` (with each column representing a predictor), and `y` is the target vector we aim to estimate. However, we still have only one estimated variable!\n",
    "\n",
    "Our equation is now:\n",
    "\n",
    "### $$ \\hat{y} = X \\beta$$\n",
    "\n",
    "In this equation, `β` is now a vector of coefficients rather than a single value. The equation can also be expressed in a different notation as:\n",
    "\n",
    "### $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n",
    "\n",
    "> **Detailed Explanation**:\n",
    "> \n",
    "> In a multiple linear regression model, we are trying to predict or estimate a target variable (what we want to know) based on several predictor variables (what we already know).\n",
    ">\n",
    "> In the formula, `X` is a matrix that contains our predictor variables. A matrix is like a table with rows and columns that holds numbers. Each column in the matrix `X` corresponds to a predictor. If we have `i` predictors, then our matrix `X` will have `i` columns.\n",
    ">\n",
    "> `β` is a vector of coefficients. Coefficients are numbers that we multiply by the predictor variables to determine how much each predictor affects the target. If the coefficient is large, that predictor has a larger effect on the target. If it's small, the predictor has a smaller effect. If we have `i` predictors, then our `β` vector will have `i` coefficients.\n",
    ">\n",
    "> So, the formula `ŷ = X β` represents the model we use to predict the target variable. `ŷ` is our predicted target variable. We calculate it by multiplying the `X` matrix by the `β` vector.\n",
    ">\n",
    "> To make this more concrete, let's say we're trying to predict someone's height (our target) based on their weight and age (our predictors). We would then have a matrix `X` with two columns (one for weight and one for age) and a `β` vector with two coefficients (one for weight and one for age).\n",
    ">\n",
    "> The formula can also be written in a different way as `ŷ = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ`. This just means that we calculate our predicted height (`ŷ`) by adding up the products of each predictor (`x`'s) and its corresponding coefficient (`β`'s).\n",
    ">\n",
    "> For instance, if we found through our model that the coefficient for weight is 0.5 and for age is 0.3, our formula would be `ŷ = β₀ + 0.5 * weight + 0.3 * age`. This means that for each unit increase in weight, the height increases by 0.5 units, and for each unit increase in age, the height increases by 0.3 units. `β₀` is a special coefficient called the intercept. It represents the estimated height when all the predictors are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the Beta Coefficients\n",
    "\n",
    "The beta coefficients are solved using the following linear algebra formula:\n",
    "\n",
    "### $$ \\beta = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "In this formula, `X^T` stands for the transposed matrix of the original matrix `X`, and `(X^TX)^-1` represents the inverted matrix of `X^TX`.\n",
    "\n",
    "> **Detailed Explanation:**\n",
    ">\n",
    "> Let's break down the formula:\n",
    "> \n",
    "> $$ \\beta = (X^TX)^{-1}X^Ty $$\n",
    "> \n",
    "> This formula helps us find the beta coefficients in multiple linear regression, which is a way of predicting a response (the thing we want to know) based on several predictors (the things we already know).\n",
    "> \n",
    "> To understand this formula, let's break it down into its parts:\n",
    "> \n",
    "> - `X`: This is a matrix that contains our predictor variables. A matrix is like a table that holds numbers. In our case, each column in this \"table\" corresponds to a predictor variable. So if we have `i` predictors, then our matrix `X` will have `i` columns. For example, let's say we're trying to predict someone's height (our response) based on their weight and age (our predictors). Then our matrix `X` would look like this:\n",
    "> \n",
    "> ```\n",
    "> Weight    Age\n",
    "> 150       30\n",
    "> 160       40\n",
    "> 170       50\n",
    "> ```\n",
    "> \n",
    "> - `X^T`: This is the transposed matrix of `X`. To transpose a matrix, we simply flip it over its diagonal, which runs from the top left to the bottom > right. So the first row becomes the first column, the second row becomes the second column, and so on. Using our previous example, the transposed matrix would look like this:\n",
    "> \n",
    "> ```\n",
    "> Weight    150    160    170\n",
    "> Age       30     40     50\n",
    "> ```\n",
    "> \n",
    "> - `X^TX`: This is the dot product of `X^T` and `X`. The dot product is a way of multiplying matrices together. To calculate the dot product of two matrices, we multiply each corresponding pair of elements (i.e., the elements that are in the same position in each matrix), and then add those products together. Here's an example:\n",
    "> \n",
    "> ```\n",
    "> Matrix A:    Matrix B:    Dot Product (A . B):\n",
    "> 1  2         4  5         (1*4 + 2*5)  = 14\n",
    "> 3  4   .     6  7   =     (3*6 + 4*7)  = 46\n",
    "> ```\n",
    "> \n",
    "> - `(X^TX)^{-1}`: This is the inverse of `X^TX`. The inverse of a matrix is like the reciprocal of a number. Just as multiplying a number by its reciprocal gives us 1, multiplying a matrix by its inverse gives us the identity matrix (a special matrix that has 1s on the diagonal and 0s everywhere else).\n",
    "> \n",
    "> > **Even Detailed Explanation:**\n",
    "> > \n",
    "> > Calculating the inverse of a matrix can be a complicated process, but thankfully we have libraries like NumPy in Python that can do it for us using the `np.linalg.inv()` function. However, it's still important to understand what's going on under the hood.\n",
    "> > \n",
    "> > In essence, the inverse of a matrix A is another matrix, denoted as A^-1, such that when you multiply A and A^-1 together, you get the identity matrix I. The identity matrix is a special type of square matrix where all the elements of the principal diagonal are ones and all other elements are zeros.\n",
    "> > \n",
    "> > Here's a simple example of a matrix and its inverse:\n",
    "> > \n",
    "> > ```python\n",
    "> > Matrix A:              Inverse of A (A^-1):\n",
    "> > 1  2                   -2   1\n",
    "> > 3  4                    1.5 -0.5\n",
    "> > ```\n",
    "> > \n",
    "> > If we multiply A by A^-1, we should get the identity matrix:\n",
    "> > \n",
    "> > ```python\n",
    "> > A * A^-1:\n",
    "> > \n",
    "> > 1  0\n",
    "> > 0  1\n",
    "> > ```\n",
    "> > \n",
    "> > In practice, finding the inverse of a matrix involves a number of steps including creating a matrix of minors, creating a matrix of cofactors, taking the transpose, and dividing by the determinant of the original matrix. It's worth noting that not all matrices have an inverse. Inverse matrices only exist for square matrices that are non-singular, i.e., their determinant is not zero.\n",
    "> > \n",
    "> > In the context of the multiple linear regression model, we use the inverse of the matrix `X^TX` to help us solve for the beta coefficients. This is because `X^TX` is a square matrix and has an inverse, assuming that the predictors are not perfectly multicollinear.\n",
    "> > \n",
    "> > In Python, we can compute the inverse of a matrix using the `np.linalg.inv()` function from the NumPy library. Here's how you might calculate the inverse of `X^TX`:\n",
    "> > \n",
    "> > ```python\n",
    "> > import numpy as np\n",
    "> > \n",
    "> > # Assuming X is your predictor matrix\n",
    "> > XTX = np.dot(X.T, X)\n",
    "> > \n",
    "> > # Calculate the inverse\n",
    "> > XTX_inv = np.linalg.inv(XTX)\n",
    "> > \n",
    "> > print(XTX_inv)\n",
    "> > \n",
    "> > ```\n",
    "> > \n",
    "> > This code will print the inverse of the `X^TX` matrix.\n",
    "> \n",
    "> - `X^Ty`: This is the dot product of `X^T` and `y`, where `y` is the actual outcomes we know from our data.\n",
    "> \n",
    "> So, putting it all together, the formula is saying that we can calculate our beta coefficients (the numbers that tell us how much each predictor affects the response) by multiplying the inverse of `X^TX` by `X^Ty`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation using the true `y` value is:\n",
    "\n",
    "### $$ y = X \\beta + \\varepsilon $$\n",
    "\n",
    "In this equation, `ε` is our vector of residuals or errors.\n",
    "\n",
    "> **Detailed Explanation:**\n",
    "> \n",
    "> Let's take a look at this equation:\n",
    "> \n",
    "> $$ y = X \\beta + \\varepsilon $$\n",
    "> \n",
    "> This equation is called a multiple linear regression equation, and it's a way to predict an outcome (the thing we want to know) based on several factors (the things we already know).\n",
    "> \n",
    "> Here's what each part of the equation means:\n",
    "> \n",
    "> 1. `y`: This is what we're trying to predict. It could be anything from the price of a house to a person's height. `y` represents the actual known outcome from our data.\n",
    "> 2. `X`: This is a matrix that represents our predictor variables. A matrix is like a table that holds numbers. Each column in this \"table\" corresponds to a predictor variable. For example, if we are trying to predict a person's weight, `X` could represent the person's height, age, and gender.\n",
    "> 3. `β`: This is a vector of coefficients. Coefficients are numbers that we multiply by the predictor variables to determine how much each predictor affects the outcome. For example, in predicting a person's weight, the coefficient for height would tell us how much each additional inch of height increases a person's weight.\n",
    "> 4. `ε`: This is a vector of residuals or errors. The residual for each prediction is the difference between the actual outcome and what our model predicted. For example, if we predict a person's weight to be 150 pounds based on their height, age, and gender, but their actual weight is 155 pounds, the residual would be 5 pounds.\n",
    "> \n",
    "> Now let's use a simple example to illustrate. Let's say we're trying to predict a person's weight (`y`) based on their height (`X`). Let's assume for simplicity that the relationship between height and weight is linear, and for every inch of height, a person weighs 5 pounds more. This is our `β` coefficient.\n",
    "> \n",
    "> So, if a person is 60 inches tall, our equation would be:\n",
    "> \n",
    "> Weight = 5 * Height + ε\n",
    "> \n",
    "> This would give us:\n",
    "> \n",
    "> Weight = 5 * 60 + ε\n",
    "> \n",
    "> But, of course, not everyone who is 60 inches tall weighs exactly 300 pounds. This is where the error term (`ε`) comes in. The error term accounts for other factors influencing a person's weight that we haven't included in our model. For example, a person's diet or exercise routine could affect their weight but isn't captured by their height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can equivalently formulate this in terms of the residuals:\n",
    "\n",
    "### $$ \\varepsilon = X \\beta - y $$\n",
    "\n",
    "The equation $ \\varepsilon = X \\beta - y $ represents the residual errors in multiple linear regression.\n",
    "\n",
    "> **Detailed Explanation:**\n",
    "> \n",
    "> Let's break down each element of this equation:\n",
    "> \n",
    "> - `ε`: This is our vector of residuals or errors. The residual for each prediction is the difference between the actual outcome and what our model predicted. For example, if we predict a person's weight to be 150 pounds based on their height, age, and gender, but their actual weight is 155 pounds, the residual would be 5 pounds.\n",
    "> - `X`: This is a matrix that represents our predictor variables. A matrix is like a table that holds numbers. Each column in this \"table\" corresponds to a predictor variable. For example, if we are trying to predict a person's weight, `X` could represent the person's height, age, and gender.\n",
    "> - `β`: This is a vector of coefficients. Coefficients are numbers that we multiply by the predictor variables to determine how much each predictor affects the outcome. For example, in predicting a person's weight, the coefficient for height would tell us how much each additional inch of height increases a person's weight.\n",
    "> - `y`: This is what we're trying to predict. It could be anything from the price of a house to a person's height. `y` represents the actual known outcome from our data.\n",
    "> \n",
    "> Let's say we're trying to predict a person's weight (`y`) based on their height (`X`). Let's assume for simplicity that the relationship between height and weight is linear, and for every inch of height, a person weighs 5 pounds more. This is our `β` coefficient.\n",
    "> \n",
    "> So, if a person is 60 inches tall, our equation would be:\n",
    "> \n",
    "> `Weight = 5 * Height + ε`\n",
    "> \n",
    "> This would give us:\n",
    "> \n",
    "> `Weight = 5 * 60 + ε`\n",
    "> \n",
    "> But, of course, not everyone who is 60 inches tall weighs exactly 300 pounds. This is where the error term (`ε`) comes in. The error term accounts for other factors influencing a person's weight that we haven't included in our model. For example, a person's diet or exercise routine could affect their weight but isn't captured by their height.\n",
    "> \n",
    "> So, if a person is 60 inches tall and weighs 280 pounds, the error term would be:\n",
    "> \n",
    "> `ε = Weight - 5 * Height`\n",
    "> \n",
    "> `ε = 280 - 5 * 60`\n",
    "> \n",
    "> `ε = -20`\n",
    "> \n",
    "> This means our model overestimated the person's weight by 20 pounds. The error term in our equation $ \\varepsilon = X \\beta - y $ represents these types of discrepancies between our predictions and the actual outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to minimize the sum of squared residuals. The sum of squared residuals is equivalent to the dot product of the vector of residuals:\n",
    "\n",
    "### $$ \\sum_{i=1}^n \\varepsilon_i^2 = \\left[\\begin{array}{cc}\\varepsilon_1 \\cdots \\varepsilon_n\\end{array}\\right]\\left[\\begin{array}{cc}\\varepsilon_1 \\\\ \\cdots \\\\ \\varepsilon_n\\end{array}\\right] = \\varepsilon^T \\varepsilon $$\n",
    "\n",
    "> **Detailed Explanation:**\n",
    "> \n",
    "> In statistics, a common goal is to make some sort of prediction based on a set of data we have. However, our predictions are rarely perfect, and there's usually some amount of error. These errors are also called 'residuals'.\n",
    "> \n",
    "> Imagine you're trying to predict your final grade in a class based on how much time you spend studying. You might create a model that says \"for every hour I study, I'll improve my grade by 5%.\" You study for 20 hours and predict you'll get a 100% in the class. But when grades come out, you actually get a 90%. Your prediction was off by 10% - that's your residual for this prediction.\n",
    "> \n",
    "> Now, let's say you make this kind of prediction for all your classes, and you have some residuals from each prediction. One way to understand how good your predictions are overall is to look at these residuals. If your residuals are small, that means your predictions are pretty good. If they're large, that means your predictions are often off by a lot.\n",
    "> \n",
    "> Here's where the 'sum of squared residuals' comes in. It's a way to add up all your residuals. Now, you might ask, why don't we just add up the residuals? Why do we square them first?\n",
    "> \n",
    "> There are a few reasons for this. One is that squaring the residuals makes sure they're all positive. If you just added them up, positive and negative residuals might cancel each other out, making it seem like your predictions are better than they actually are. Squaring the residuals also emphasizes larger errors. A residual of 5 becomes 25 when squared, and a residual of 10 becomes 100. This means that predictions that are way off will greatly increase the sum of squared residuals.\n",
    "> \n",
    "> The equation you see, $$ \\sum_{i=1}^n \\varepsilon_i^2 = \\left[\\begin{array}{cc}\\varepsilon_1 \\cdots \\varepsilon_n\\end{array}\\right]\\left[\\begin{array}{cc}\\varepsilon_1 \\\\ \\cdots \\\\ \\varepsilon_n\\end{array}\\right] = \\varepsilon^T \\varepsilon $$, is a mathematical way to represent this sum of squared residuals.\n",
    "> \n",
    "> Let's break that down:\n",
    "> \n",
    "> - $ \\sum_{i=1}^n \\varepsilon_i^2 $ This is a way to represent the sum of squared residuals. The `ε` represents the residuals, and the `i` is an index that goes from 1 to `n`, where `n` is the total number of residuals we have. So $ \\sum_{i=1}^n \\varepsilon_i^2 $ means \"square each residual and add them all up.\"\n",
    "> - $ \\left[\\begin{array}{cc}\\varepsilon_1 \\cdots \\varepsilon_n\\end{array}\\right]\\left[\\begin{array}{cc}\\varepsilon_1 \\\\ \\cdots \\\\ \\varepsilon_n\\end{array}\\right] $ This is another way to represent the same thing. Here, we're treating the residuals as a vector - a list of numbers - and multiplying the vector by itself. In this context, the multiplication is a kind of special multiplication called a dot product, which means \"multiply corresponding elements and then add them up.\" Since we're multiplying each residual by itself, that's the same as squaring them, and since we're adding those up, that's the same as our sum of squared residuals.\n",
    "> - $ \\varepsilon^T \\varepsilon $ This is yet another way to represent the sum of squared residuals. The 'T' superscript stands for 'transpose', which is a fancy way to say \"turn rows into columns and columns into rows\". For our purposes, though, you can think of this as just another way to say \"multiply the vector of residuals by itself and add up the results\", which, as we've seen, is the same as the sum of squared residuals.\n",
    "> \n",
    "> So, all three parts of this equation are saying the same thing in different mathematical languages: \"square each residual, add them up, and that's your sum of squared residuals.\"\n",
    "> \n",
    "> Our goal in creating a prediction model is to make this sum of squared residuals as small as possible because that would mean our predictions are close to the actual outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can write the sum of squared residuals as:\n",
    "\n",
    "### $$ \\varepsilon^T \\varepsilon = (X \\beta - y)^T (X \\beta - y) $$\n",
    "\n",
    "> **Detailed Explanation:**\n",
    "> \n",
    "> First, let's understand what residuals are in a simple context. Let's say you're trying to guess the weight of your friend's backpack. You guess that it weighs 10 pounds, but when you actually weigh it, it comes out to be 12 pounds. The difference between your guess (10 pounds) and the actual weight (12 pounds) is called a 'residual'. In this case, your residual would be 12 - 10 = 2 pounds.\n",
    "> \n",
    "> Now, let's say you made a lot of guesses like this for different backpacks, and you want to know how good your guesses were overall. One way to do this is to calculate the 'sum of squared residuals'. Here's how you do that:\n",
    "> \n",
    "> 1. For each guess, you calculate the residual (the difference between your guess and the actual value).\n",
    "> 2. You square each residual (multiply it by itself).\n",
    "> 3. You add up all these squared residuals.\n",
    "> \n",
    "> The reason you square each residual is to make sure they're all positive. If you just added up the residuals without squaring them, then positive and negative residuals could cancel each other out, which could mislead you into thinking your guesses were better than they actually were. Also, squaring the residuals punishes larger errors more than smaller ones, which is generally what we want.\n",
    "> \n",
    "> Now let's try to understand the equation:\n",
    "> \n",
    "> $$ \\varepsilon^T \\varepsilon = (X \\beta - y)^T (X \\beta - y) $$\n",
    "> \n",
    "> This equation is a formal way of representing the 'sum of squared residuals' in the context of multiple linear regression, which is a way of making predictions based on several factors.\n",
    "> \n",
    "> Let's break down each part of this equation:\n",
    "> \n",
    "> 1. $ \\varepsilon^T \\varepsilon $: This represents the sum of squared residuals. The 'T' stands for 'transpose', which is a fancy way of saying \"flip the rows and columns\". In this context, think of it as a way of saying \"calculate the sum of squared residuals\".\n",
    "> 2. $ (X \\beta - y)^T (X \\beta - y) $: This is another way of expressing the same thing. Here, 'X' represents the factors you're using to make your predictions, 'β' represents the 'weights' or importance you're giving to each factor, and 'y' represents the actual values you're trying to predict.\n",
    "> \n",
    "> Let's break this down further with an example:\n",
    "> \n",
    "> Let's say you're trying to predict how much a house will sell for based on its size and location. Here, 'X' would be a table that lists the size and location of each house, 'β' would be how much you think size and location affect the house price, and 'y' would be the actual selling prices of the houses.\n",
    "> \n",
    "> So, $ X \\beta $ is your guess for each house's price, based on its size and location. $ X \\beta - y $ is the residual for each guess (the difference between your guess and the actual selling price).\n",
    "> \n",
    "> By squaring each of these residuals ($ (X \\beta - y)^T (X \\beta - y) $), and adding them up ($ \\varepsilon^T \\varepsilon $), you get the sum of squared residuals, which tells you how good your guesses were overall.\n",
    "> \n",
    "> So, all in all, this equation is a mathematical way of saying \"calculate the sum of squared residuals for our predictions\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which simplifies to:\n",
    "\n",
    "### $$ \\varepsilon^T \\varepsilon = y^Ty - y^TX\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta $$\n",
    "\n",
    "> **Detailed Explanation:**\n",
    "> \n",
    "> The left side of the equation, `ε^T ε`, represents the sum of the squares of the residuals. A residual, in this case, is the difference between the actual value and the value predicted by our model. The symbol `^T` denotes a transpose operation, which basically means flipping the rows and columns of a matrix (think of it as switching the places of all elements of a matrix). When you see `ε^T ε`, it means the sum of each residual squared.\n",
    "> \n",
    "> Now, let's dissect the right side of the equation:\n",
    "> \n",
    "> - `y^Ty`: This is the sum of the square of all actual values.\n",
    "> - `β^T X^T y`: This is the same as `y^TXβ`. In multiplication, the order usually doesn't matter (5 * 4 is the same as 4 * 5), which is why these two are the same.\n",
    "> - `β^T X^T X β`: This is the sum of the product of each predicted value and itself.\n",
    "> \n",
    "> Now let's illustrate this with a simple example. Let's say we have a model that predicts a student's test score based on the number of hours they study. Here's some data:\n",
    "> \n",
    "> | Hours Studied (X) | Actual Score (y) | Predicted Score (Xβ) |\n",
    "> | --- | --- | --- |\n",
    "> | 2 | 60 | 50 |\n",
    "> | 4 | 80 | 70 |\n",
    "> | 6 | 90 | 85 |\n",
    "> \n",
    "> Let's break down the right side of the equation:\n",
    "> \n",
    "> - `y^Ty` = 60^2 + 80^2 + 90^2 = 3600 + 6400 + 8100 = 18100\n",
    "> - `y^TXβ` = 60 * 50 + 80 * 70 + 90 * 85 = 3000 + 5600 + 7650 = 16250\n",
    "> - `β^T X^T y` = 50 * 60 + 70 * 80 + 85 * 90 = 3000 + 5600 + 7650 = 16250\n",
    "> - `β^T X^T X β` = 50^2 + 70^2 + 85^2 = 2500 + 4900 + 7225 = 14625\n",
    "> \n",
    "> So, according to the equation, the sum of squares of the residuals `ε^T ε` is equal to `y^Ty - y^TXβ - β^T X^T y + β^T X^T X β`, or 18100 - 16250 - 16250 + 14625 = 225.\n",
    "> \n",
    "> The residuals (the differences between actual and predicted scores) are 10, 10 and 5. Their squares are 100, 100 and 25, and if you add these up, you get 225, which matches with our earlier calculation.\n",
    "> \n",
    "> In conclusion, this complicated-looking equation is basically saying that if you take the sum of the squares of the actual scores, subtract twice the sum of the product of actual scores and their predicted counterparts, and add the sum of the squares of the predicted scores, you will get the sum of the squares of the differences between the actual and predicted scores (the residuals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will take the derivative with respect to `β`:\n",
    "\n",
    "### $$ \\frac{\\partial \\varepsilon^T \\varepsilon}{\\partial \\beta} = - 2X^Ty + 2X^TX\\beta$$\n",
    "\n",
    "> **Detailed Explanation:**\n",
    "> \n",
    "> When you see a graph of a function (like the graph of a line or a curve), the derivative at a certain point is the slope of the line that just touches the graph at that point (this line is also known as the tangent line). If the graph is a straight line, then the derivative is just the slope of that line, and it's the same at every point. But if the graph is a curve, then the derivative can be different at different points.\n",
    "> \n",
    "> The derivative can be thought of as a rate of change. For example, if you have a function that describes how far a car has traveled over time, the derivative of that function at a certain point would give you the speed of the car at that time.\n",
    "> \n",
    "> In the context of our equation, we're taking the derivative with respect to `β`, which means we're looking at how our function changes as `β` changes. This is useful because it can help us find the value of `β` that minimizes the function, which is often a goal in machine learning and statistics.\n",
    "> \n",
    "> Now, let's look at the equation itself:\n",
    "> \n",
    "> ### $$ \\frac{\\partial \\varepsilon^T \\varepsilon}{\\partial \\beta} = - 2X^Ty + 2X^TX\\beta$$\n",
    "> \n",
    "> This equation is telling us the derivative of the function `ε^T ε` with respect to `β`. Here's a breakdown of each part:\n",
    "> \n",
    "> - `ε^T ε`: This represents the sum of the squares of the residuals, which are the differences between the actual and predicted values.\n",
    "> - `X`: This is a matrix that represents our predictor variables, such as the number of hours studied.\n",
    "> - `y`: This is a vector that represents the actual outcomes, such as the actual scores on the test.\n",
    "> - `β`: This is a vector of coefficients that we're trying to solve for. These coefficients tell us how much each predictor affects the outcome.\n",
    "> \n",
    "> Now let's break down the right-hand side of the equation:\n",
    "> \n",
    "> - `2X^Ty`: This represents the derivative of the part of the function `y^TXβ - β^T X^T y` with respect to `β`. The `2` comes from the power rule of derivatives, which states that the derivative of `x^n` is `n*x^(n-1)`. Here, because the power is `2` (because we're dealing with squares in the original function), the `2` comes down in front when we take the derivative. The `X^Ty` part is the derivative of `y^TXβ` with respect to `β`.\n",
    "> - The negative sign in `-2X^Ty` comes from the application of the chain rule in differentiation. When we differentiate the term `y^TXβ` with respect to `β`, we apply the chain rule because the term `y^TXβ` is a function of `β`. In this context, the chain rule states that the derivative of a function of a function is the derivative of the outer function times the derivative of the inner function.\n",
    "> In the term `y^TXβ`, `Xβ` is the inner function and `y^T` is the outer function. The derivative of the outer function `y^T` is a constant since it does not involve `β`. The derivative of the inner function `Xβ` with respect to `β` is `X`.\n",
    "> However, when we take the derivative of the entire function `ε^T ε` with respect to `β`, we are considering the change in the residuals as `β` changes. As `β` increases, our predicted values increase, making the residuals smaller (since residuals are `actual - predicted`). Therefore, the change in the residuals is negative, leading to the negative sign in `-2X^Ty`.\n",
    "> - `+ 2X^TXβ`: This represents the derivative of the part of the function `β^T X^T X β` with respect to `β`. Again, the `2` comes from the power rule of derivatives, and the `X^TXβ` part is the derivative of `β^T X^T X β` with respect to `β`.\n",
    "> \n",
    "> So, this equation is telling us how our function `ε^T ε` changes as we change the coefficients `β`. We can use this information to find the coefficients that minimize the sum of the squares of the residuals, which is often our goal in machine learning and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the sum of squared errors, we set the derivative to zero and solve for the beta coefficient vector:\n",
    "\n",
    "### $$ 0 = -2X^Ty + 2X^TX\\beta $$\n",
    "### $$ X^TX\\beta = X^Ty $$\n",
    "### $$ \\beta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "> **Detailed Explanation:**\n",
    "> \n",
    "> Let's go step by step:\n",
    "> \n",
    "> 1. **$$ 0 = -2X^Ty + 2X^TX\\beta $$**: This equation is a result of finding the minimum of a function (the sum of squared residuals) by taking the derivative and setting it to zero. The reason we want to minimize the sum of squared residuals is because this will give us the most accurate predictions. The residuals represent the difference between the actual data and our predictions. So, by minimizing the sum of these squared residuals, we're finding the best possible fit for our data. This is like trying to draw the straightest possible line through a set of points on a graph. The \"best\" line is the one where the sum of the squares of the vertical distances of each point from the line is the smallest.\n",
    "> 2. **$$ X^TX\\beta = X^Ty $$**: This equation is a simplification of the first equation. We've removed the zero and the 2 for simplification. This equation is saying that the product of our factors (X) and our unknown coefficients (β) is equal to the product of our factors (X) and our known outcomes (y). This becomes an equation that we can solve to find β, which are the coefficients that determine how each factor affects the outcome.\n",
    "> 3. **$$ \\beta = (X^TX)^{-1}X^Ty$$**: Finally, we solve the equation for β. The term \"X^TX\" is a square matrix (a table of numbers with the same number of rows and columns), and \"(X^TX)^{-1}\" represents its inverse (which is like its reciprocal). So, this equation is saying that the coefficients we're looking for are equal to the inverse of our factors (X) multiplied by themselves, then multiplied by our factors (X) and our known outcomes (y).\n",
    "> \n",
    "> In simpler terms, these equations help us find the best possible way to predict an outcome based on several factors. By \"best,\" we mean that the predictions made using these equations are as close as possible to the actual outcomes.\n",
    "> \n",
    "> For example, let's say we're trying to predict a student's final exam score (y) based on the number of hours they studied (X1), and the number of classes they attended (X2). The equations above would help us find the coefficients (β1 and β2) that tell us how much each hour of study and each class attended contributes to the final exam score. These coefficients allow us to make predictions that are as accurate as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implementing Multiple Linear Regression\n",
    "\n",
    "First, we need to construct what is known as a \"design matrix\" for our predictors. The design matrix is a two-dimensional matrix that contains our predictor variables.\n",
    "\n",
    "The first column of the design matrix should be a column of all 1s. This column represents the intercept term of our regression model.\n",
    "\n",
    "Following this, the subsequent columns should contain the values of our predictor variables. In our case, these predictors are `carat`, `depth`, `table`, `x`, `y`, and `z`.\n",
    "\n",
    "Constructing the design matrix is straightforward with the help of pandas, a Python library for data manipulation and analysis. We first add a column for the intercept to our DataFrame. We then convert the DataFrame to a matrix with the `.values` attribute. This will leave us with a design matrix that is ready for use in our multiple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23 61.5  55.    3.95  3.98  2.43  1.  ]\n",
      " [ 0.21 59.8  61.    3.89  3.84  2.31  1.  ]\n",
      " [ 0.23 56.9  65.    4.05  4.07  2.31  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# First, we're creating a new DataFrame, X, that contains all of the rows from the 'carat', 'depth', 'table', 'x', 'y', and 'z' columns of the 'diamonds' DataFrame.\n",
    "# The '.loc' function is used to access a group of rows and columns by label(s) or a boolean array,\n",
    "# ':' indicates all rows, and the list after the comma indicates the columns we are interested in.\n",
    "X = diamonds.loc[:,['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
    "\n",
    "# Next, we're adding a new column to the 'X' DataFrame called 'intercept', and setting all of its values to 1.0.\n",
    "# This is necessary for the multiple linear regression model, as it represents the y-intercept of the regression equation.\n",
    "X['intercept'] = 1.\n",
    "\n",
    "# Then, we're converting the 'X' DataFrame into a numpy array using the '.values' attribute.\n",
    "# This is necessary because the multiple linear regression model requires our input data to be in the form of a numpy array.\n",
    "X = X.values\n",
    "\n",
    "# Finally, we're printing the first three rows of the 'X' array to the console.\n",
    "# This is done to give us a quick look at our prepared data before we feed it into the multiple linear regression model.\n",
    "print(X[0:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for the Beta Coefficients\n",
    "\n",
    "In this step, we're aiming to calculate the beta coefficients for our multiple linear regression model. Recall that we are predicting `price` based on our selected predictors.\n",
    "\n",
    "The equation to solve for the beta coefficients is:\n",
    "\n",
    "### $$ \\beta = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "Here's a breakdown of how to implement this equation in Python:\n",
    "\n",
    "**Transposing a Matrix:**\n",
    "\n",
    "The transpose of a matrix is calculated by appending `.T` to the matrix. In our context, `X.T` would give us the transposed matrix of `X`.\n",
    "\n",
    "**Dot Product of Matrices:**\n",
    "\n",
    "The dot product of two matrices is calculated using `np.dot(mat1, mat2)`. In our equation, we need to calculate the dot product for two instances: `X^TX` and `X^Ty`.\n",
    "\n",
    "**Inverting a Matrix:**\n",
    "\n",
    "The inversion of a matrix is done using `np.linalg.inv()`. In our equation, we need to calculate the inversion of `X^TX`, which is represented as `(X^TX)^−1`.\n",
    "\n",
    "In conclusion, we need to plug in our matrices into the equation and perform these operations to get the beta coefficients for our multiple linear regression model. These coefficients will help us understand the relationship between our predictors and the `price`. Please remember to ensure that all matrices are in the correct dimensions before performing these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10686.30908063  -203.1540524   -102.44565213 -1315.66784181\n",
      "    66.32160232    41.62769702 20849.31641311]\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieving the 'price' column from the diamonds DataFrame and converting it to a numpy array\n",
    "# This is our target variable that we are trying to predict\n",
    "price: np.ndarray = diamonds['price'].values\n",
    "\n",
    "# The beta coefficients are calculated using the following formula: beta = (X^TX)^-1X^Ty\n",
    "# np.linalg.inv() is used to calculate the inverse of a matrix\n",
    "# np.dot() is used for matrix multiplication\n",
    "# X.T is the transpose of X\n",
    "# So, np.dot(X.T, X) is the dot product of X and its transpose\n",
    "# np.linalg.inv(np.dot(X.T, X)) is the inverse of the dot product of X and its transpose\n",
    "# np.dot(np.linalg.inv(np.dot(X.T, X)), X.T) is the dot product of the inverse of the dot product of X and its transpose, and the transpose of X\n",
    "# np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), price) is the dot product of the previous result and the price array\n",
    "# This gives the beta coefficients\n",
    "beta_vec: np.ndarray = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), price)\n",
    "\n",
    "# Printing the beta coefficients\n",
    "print(beta_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the predicted values, denoted as $\\hat{y}$, using the predictor matrix `X` and the $\\beta$ coefficients that you've computed, you will perform a matrix multiplication.\n",
    "\n",
    "This means you'll multiply the `X` predictor matrix by the $\\beta$ coefficients. In Python, you'd typically do this using the `np.dot()` function from the numpy library, which performs the dot product of two arrays.\n",
    "\n",
    "The formula is $\\hat{y} = X \\beta$, where $X$ is your predictor matrix and $\\beta$ is the set of coefficients.\n",
    "\n",
    "In Python, if `X` is your predictor matrix and `beta_vec` is your vector of $\\beta$ coefficients, the code to accomplish this might look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 346.90971807,  -71.46876469,  126.3686742 , ..., 2503.1404693 ,\n",
       "       4175.51576877, 3463.84440251])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = np.dot(X, beta_vec)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `y_hat` is a vector of predicted values for your dependent variable, based on the multiple linear regression model you've built with the predictor matrix `X` and the $\\beta$ coefficients.\n",
    "\n",
    "To calculate the Root Mean Squared Error (RMSE) of the multiple regression model, follow these steps:\n",
    "\n",
    "1. Compute the Residuals: Residuals are the difference between the actual values and the predicted values (y - yhat). You can calculate this by subtracting the predicted values (yhat) from the actual y values.\n",
    "2. Square the Residuals: For each residual, square the value (residual^2). Squaring is necessary to remove any negative signs. It also emphasizes larger differences.\n",
    "3. Calculate the Mean of the Squared Residuals: Sum all the squared residuals then divide by the number of residuals. This is known as the Mean Squared Error (MSE).\n",
    "4. Take the Square Root of the Mean Squared Error: This is the final step in calculating RMSE. The square root is taken to bring our error term back to the same units as our original data. The resulting value represents the standard deviation of the residuals.\n",
    "\n",
    "Lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response, and is the most important criterion for fit if the main purpose of the model is prediction.\n",
    "\n",
    "Here is how you can do it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1496.8572842935746\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "residuals = price - y_hat\n",
    "squared_residuals = np.square(residuals)\n",
    "MSE = np.mean(squared_residuals)\n",
    "RMSE = np.sqrt(MSE)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Create a `MultipleLinearRegression` class that has the following properties:\n",
    "\n",
    "- A constructor that takes two parameters: a 2D numpy array `X` representing the matrix of predictor variables and a 1D numpy array `y` representing the target variable.\n",
    "- A `fit` method which calculates the beta coefficients of the multiple linear regression model using the formula `β = (X^TX)^{-1}X^Ty`.\n",
    "- A `predict` method which takes a 2D numpy array representing the predictor variables of new data points and returns a 1D array of predicted target variables for these data points.\n",
    "\n",
    "### Output Format\n",
    "\n",
    "- The `fit` method must return a 1D numpy array representing the beta coefficients of the multiple linear regression model.\n",
    "- The `predict` method must return a 1D numpy array representing the predicted target values.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Consider the following code:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Create predictor matrix and target variable\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([2, 3, 4])\n",
    "\n",
    "# Create a multiple linear regression model\n",
    "model = MultipleLinearRegression(X, y)\n",
    "\n",
    "# Fit the model\n",
    "beta_vec = model.fit()\n",
    "print(beta_vec)\n",
    "\n",
    "# Predict the target value of a new data point\n",
    "X_new = np.array([[10, 11, 12]])\n",
    "y_pred = model.predict(X_new)\n",
    "print(y_pred)\n",
    "\n",
    "```\n",
    "\n",
    "When executed with a properly implemented `MultipleLinearRegression` class, this code should print the beta coefficients of the multiple linear regression model and the predicted target value of the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE BELOW THIS LINE ###\n",
    "\n",
    "\n",
    "### WRITE YOUR CODE ABOVE THIS LINE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
