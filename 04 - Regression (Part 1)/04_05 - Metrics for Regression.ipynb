{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04.05 - Metrics for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression analysis, we have several metrics that allow us to compare the performance of different models, including an adjusted model versus a baseline model. These metrics quantify the difference between the predicted values and the actual values, giving us an idea of how well our model is performing. It's important to note that in this discussion, we'll focus on the implementation using PyTorch, a machine learning library for the Python programming language.\n",
    "\n",
    "One common metric is Mean Squared Error (MSE). This metric takes the average of the squared differences between the predicted and actual values. The squaring is significant because it removes the direction of the error, focusing solely on the magnitude. In PyTorch, you can compute the MSE using the `torch.nn.MSELoss()` function.\n",
    "\n",
    "Another metric is Root Mean Squared Error (RMSE), which is the square root of the MSE. Taking the square root is useful because it brings the error metric back to the same units as the target variable, making it more interpretable. You can compute the RMSE in PyTorch by first calculating the MSE and then taking the square root using `torch.sqrt()`.\n",
    "\n",
    "Mean Absolute Error (MAE) is another commonly used metric. It calculates the average absolute difference between the predicted and actual values. This can be particularly useful if you want to know the size of the error, but don't care whether it's over or under-prediction. You can compute the MAE in PyTorch using the `torch.nn.L1Loss()` function.\n",
    "\n",
    "We also often use R-squared (R²) to compare an adjusted model to a baseline. R-squared quantifies the proportion of the variance in the target variable that is predictable from the input variables. A higher R-squared indicates a better fit to the data. While PyTorch doesn't have a built-in function for R-squared, you can calculate it manually by first computing the total sum of squares and the residual sum of squares.\n",
    "\n",
    "Remember, no single metric can tell the whole story. It's important to consider these metrics in conjunction, and in context of the specific problem and dataset you're working with.\n",
    "\n",
    "## Train a Simple Linear Regression Model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 31386426.0\n",
      "Epoch 100, Loss 29866386.0\n",
      "Epoch 200, Loss 28445910.0\n",
      "Epoch 300, Loss 27118346.0\n",
      "Epoch 400, Loss 25877488.0\n",
      "Epoch 500, Loss 24717536.0\n",
      "Epoch 600, Loss 23633092.0\n",
      "Epoch 700, Loss 22619116.0\n",
      "Epoch 800, Loss 21670894.0\n",
      "Epoch 900, Loss 20784046.0\n",
      "Epoch 1000, Loss 19954470.0\n",
      "Epoch 1100, Loss 19178342.0\n",
      "Epoch 1200, Loss 18452098.0\n",
      "Epoch 1300, Loss 17772408.0\n",
      "Epoch 1400, Loss 17136170.0\n",
      "Epoch 1500, Loss 16540483.0\n",
      "Epoch 1600, Loss 15982638.0\n",
      "Epoch 1700, Loss 15460123.0\n",
      "Epoch 1800, Loss 14970578.0\n",
      "Epoch 1900, Loss 14511807.0\n",
      "Epoch 2000, Loss 14081762.0\n",
      "Epoch 2100, Loss 13678531.0\n",
      "Epoch 2200, Loss 13300329.0\n",
      "Epoch 2300, Loss 12945487.0\n",
      "Epoch 2400, Loss 12612458.0\n",
      "Epoch 2500, Loss 12299786.0\n",
      "Epoch 2600, Loss 12006125.0\n",
      "Epoch 2700, Loss 11730203.0\n",
      "Epoch 2800, Loss 11470850.0\n",
      "Epoch 2900, Loss 11226967.0\n",
      "Epoch 3000, Loss 10997527.0\n",
      "Epoch 3100, Loss 10781570.0\n",
      "Epoch 3200, Loss 10578207.0\n",
      "Epoch 3300, Loss 10386606.0\n",
      "Epoch 3400, Loss 10205983.0\n",
      "Epoch 3500, Loss 10035620.0\n",
      "Epoch 3600, Loss 9874832.0\n",
      "Epoch 3700, Loss 9722995.0\n",
      "Epoch 3800, Loss 9579517.0\n",
      "Epoch 3900, Loss 9443845.0\n",
      "Epoch 4000, Loss 9315469.0\n",
      "Epoch 4100, Loss 9193910.0\n",
      "Epoch 4200, Loss 9078718.0\n",
      "Epoch 4300, Loss 8969477.0\n",
      "Epoch 4400, Loss 8865800.0\n",
      "Epoch 4500, Loss 8767321.0\n",
      "Epoch 4600, Loss 8673703.0\n",
      "Epoch 4700, Loss 8584623.0\n",
      "Epoch 4800, Loss 8499798.0\n",
      "Epoch 4900, Loss 8418946.0\n",
      "\n",
      "Final loss: 8342563.5\n",
      "Beta coefficients (weights): 2640.271484375\n",
      "Bias: 1575.45751953125\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Path to the diamonds.csv file\n",
    "diamonds_csv: str = './diamonds.csv'\n",
    "\n",
    "# Reading the csv file into pandas DataFrame\n",
    "diamonds: pd.DataFrame = pd.read_csv(diamonds_csv)\n",
    "\n",
    "# Selecting the features for the model\n",
    "df: pd.DataFrame = pd.DataFrame(diamonds.loc[:,['carat', 'depth', 'table', 'x', 'y', 'z']], columns=['carat', 'depth', 'table', 'x', 'y', 'z'])\n",
    "\n",
    "# Extracting the target variable (price) from the original dataset\n",
    "price: np.ndarray = diamonds['price'].values\n",
    "target: pd.DataFrame = pd.DataFrame(price)\n",
    "\n",
    "# Determining the device to be used for computations (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \\\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \\\n",
    "                      \"cpu\")\n",
    "\n",
    "# Converting the target and features to tensors and moving them to the appropriate device\n",
    "y = torch.tensor(target.values).float().to(device)\n",
    "y = y.view(y.shape[0], 1)\n",
    "X = torch.tensor(df[['carat']].values).float().to(device)\n",
    "\n",
    "# Defining the model (a simple linear regression)\n",
    "model = torch.nn.Linear(in_features = 1, out_features = 1).to(device)\n",
    "\n",
    "# Defining the loss function (Mean Squared Error) and the optimizer (SGD)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training the model for 5000 epochs\n",
    "for epoch in range(5000):\n",
    "    # Forward pass: compute predicted y by passing x to the model\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss {loss.item()}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Print final loss, weights and bias\n",
    "print(f'\\nFinal loss: {loss.item()}')\n",
    "print(f'Beta coefficients (weights): {model.weight.item()}')\n",
    "print(f'Bias: {model.bias.item()}')\n",
    "\n",
    "# Getting the model's predictions and converting them back to a numpy array\n",
    "predictions = model.cpu()(X.cpu()).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE)\n",
    "\n",
    "Mean Absolute Error (MAE) is another metric used in regression analysis to evaluate the performance of a model. It is calculated as the average absolute difference between the actual target values and the predicted values from our model. The formula for MAE is:\n",
    "\n",
    "```\n",
    "MAE = (1/n) * Σ|actual - prediction|\n",
    "\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `n` is the total number of data points\n",
    "- `Σ` represents the sum\n",
    "- `actual` is the actual target value\n",
    "- `prediction` is the predicted value from our model\n",
    "\n",
    "In PyTorch, we can calculate the MAE using the `torch.nn.L1Loss()` function, which computes the mean absolute error between the predicted and actual values.\n",
    "\n",
    "After you've trained your linear regression model and generated predictions, you can calculate the MAE as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 2012.4505615234375\n"
     ]
    }
   ],
   "source": [
    "# Defining the loss function (Mean Absolute Error)\n",
    "loss_function_mae = torch.nn.L1Loss()\n",
    "\n",
    "# Compute the Mean Absolute Error\n",
    "mae = loss_function_mae(torch.tensor(predictions), y.cpu())\n",
    "print(f'Mean Absolute Error: {mae.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key features of MAE is that it provides a linear score, meaning all individual differences are weighted equally in the average. However, this could be a limitation if you have outliers in your data, as MAE does not make use of the direction of the error and does not penalize large errors as much as Mean Squared Error does. Therefore, if large errors are particularly undesirable in your specific use case, MAE might not be the best metric to use.\n",
    "\n",
    "## Residual Sum of Squares (RSS)\n",
    "\n",
    "Residual Sum of Squares (RSS) is another important metric used in regression analysis. It is the sum of the squares of the residuals, which are the differences between the actual and predicted values. The formula for RSS is:\n",
    "\n",
    "```\n",
    "RSS = Σ(actual - prediction)^2\n",
    "\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `Σ` represents the sum\n",
    "- `actual` is the actual target value\n",
    "- `prediction` is the predicted value from our model\n",
    "\n",
    "In PyTorch, although there isn't a built-in function to compute RSS, it can be calculated easily using basic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Sum of Squares: 449957199872.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the Residual Sum of Squares\n",
    "residuals = y.cpu() - torch.tensor(predictions)\n",
    "rss = torch.sum(residuals**2)\n",
    "print(f'Residual Sum of Squares: {rss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the limitations of the RSS metric is that it heavily penalizes larger errors due to the squaring operation. This means that even if your model performs well on a majority of predictions, a few large errors can significantly increase the RSS, suggesting a worse fit than may be the case. Furthermore, RSS is scale-dependent — it depends on the scale of your target variable. Consequently, comparing the RSS between different datasets or even between different target variables within the same dataset can be misleading.\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "\n",
    "Mean Squared Error (MSE) is a frequently used regression metric that assesses the quality of a model by calculating the average of squared differences between actual and predicted values. The formula for MSE is:\n",
    "\n",
    "```\n",
    "MSE = (1/n) * Σ(actual - prediction)^2\n",
    "\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `n` is the total number of data points\n",
    "- `Σ` represents the sum\n",
    "- `actual` is the actual target value\n",
    "- `prediction` is the predicted value from our model\n",
    "\n",
    "The squaring operation within the formula ensures that each term is positive and emphasizes larger errors over smaller ones, which can be valuable when larger errors are particularly undesirable.\n",
    "\n",
    "In PyTorch, you can calculate the MSE using the `torch.nn.MSELoss()` function. After training your linear regression model and making predictions, you can compute the MSE as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 8341809.5\n"
     ]
    }
   ],
   "source": [
    "# Defining the loss function (Mean Squared Error)\n",
    "loss_function_mse = torch.nn.MSELoss()\n",
    "\n",
    "# Compute the Mean Squared Error\n",
    "mse = loss_function_mse(torch.tensor(predictions), y.cpu())\n",
    "print(f'Mean Squared Error: {mse.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While MSE is a useful metric, it's not without limitations. One significant drawback is that because the errors are squared before they're averaged, MSE places a heavier penalty on larger errors. This means that even a single outlier can significantly skew the MSE, suggesting a worse fit than may actually be the case. Additionally, because MSE squares the units of the target variable, the resulting metric can sometimes be challenging to interpret intuitively. It's also worth noting that, like RSS, MSE is scale-dependent, so comparing the MSE between different datasets or different target variables within the same dataset can be misleading.\n",
    "\n",
    "## Root Mean Squared Error (RMSE)\n",
    "\n",
    "Root Mean Squared Error (RMSE) is a standard way to measure the error of a model in predicting quantitative data. Formally, it is the square root of the average of squared differences between prediction and actual observation. The formula for RMSE is:\n",
    "\n",
    "```\n",
    "RMSE = sqrt[(1/n) * Σ(actual - prediction)^2]\n",
    "\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `n` is the total number of data points\n",
    "- `Σ` represents the sum\n",
    "- `actual` is the actual target value\n",
    "- `prediction` is the predicted value from our model\n",
    "\n",
    "The square root operation is useful because it brings the error metric back to the same units as the target variable, making it more interpretable than the Mean Squared Error (MSE).\n",
    "\n",
    "Let's compute RMSE in PyTorch using the predictions from our linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 2888.218994140625\n"
     ]
    }
   ],
   "source": [
    "# Compute the Root Mean Squared Error\n",
    "rmse = torch.sqrt(torch.mean((y.cpu() - torch.tensor(predictions))**2))\n",
    "print(f'Root Mean Squared Error: {rmse.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While RMSE is a useful metric, it's not without limitations. Similar to MSE, RMSE also squares the error before taking the square root, placing a heavier penalty on larger errors. This means that even a single outlier can significantly skew the RMSE, suggesting a worse fit than may actually be the case. Additionally, because RMSE is scale-dependent, so comparing the RMSE between different datasets or different target variables within the same dataset can be misleading.\n",
    "\n",
    "## Coefficient of Determination (R-Squared)\n",
    "\n",
    "The Coefficient of Determination, also known as R-squared (R²), is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. The formula for R-squared is:\n",
    "\n",
    "```\n",
    "R² = 1 - (RSS/TSS)\n",
    "\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `RSS` is the residual sum of squares\n",
    "- `TSS` is the total sum of squares\n",
    "\n",
    "In PyTorch, while there isn't a built-in function to compute R-squared, you can calculate it manually as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared: 0.47586339712142944\n"
     ]
    }
   ],
   "source": [
    "# Compute the total sum of squares\n",
    "tss = torch.sum((y.cpu() - torch.mean(y.cpu()))**2)\n",
    "\n",
    "# Compute the residual sum of squares\n",
    "residuals = y.cpu() - torch.tensor(predictions)\n",
    "rss = torch.sum(residuals**2)\n",
    "\n",
    "# Compute R-squared\n",
    "r_squared = 1 - (rss / tss)\n",
    "print(f'R-Squared: {r_squared.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared ranges from 0 to 1. An R-squared of 100 percent indicates that all changes in the dependent variable are completely explained by changes in the independent variable(s). Conversely, an R-squared of 0 percent indicates that the model explains none of the variability of the response data around its mean.\n",
    "\n",
    "However, R-squared is not without its limitations. A high R-squared does not necessarily indicate a good fit to the data, and conversely, a low R-squared is not necessarily indicative of a poor fit. R-squared does not indicate whether a regression model is adequate. You can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data. It also does not provide a formal hypothesis test for the adequacy of the model. Furthermore, R-squared cannot determine whether the coefficient estimates and predictions are biased.\n",
    "\n",
    "## Adjusted R-Squared\n",
    "\n",
    "Adjusted R-squared is a modification of R-squared that adjusts for the number of predictors in a model. Unlike R-squared, the adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it's usually not. It is always lower than the R-squared.\n",
    "\n",
    "The formula for adjusted R-squared is:\n",
    "\n",
    "```\n",
    "Adjusted R² = 1 - [(1-R²)*(n-1)/(n-p-1)]\n",
    "\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `n` is the total number of data points\n",
    "- `p` is the number of predictors\n",
    "\n",
    "The adjusted R-squared compensates for the addition of variables and only increases if the new variable improves the model above what would be expected by chance. It takes into account the degrees of freedom (essentially, the number of variables) and adjusts its score accordingly.\n",
    "\n",
    "It is particularly useful when comparing the performance of different regression models in predicting the same outcome variable: while a model with more predictors may have a higher R-squared, adjusted R-squared allows you to compare these models on even footing.\n",
    "\n",
    "In PyTorch, while there isn't a built-in function to compute adjusted R-squared, you can calculate it manually as follows. Let's say `p` is the number of predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-Squared: 0.4758536219596863\n"
     ]
    }
   ],
   "source": [
    "# Compute the total sum of squares\n",
    "tss = torch.sum((y.cpu() - torch.mean(y.cpu()))**2)\n",
    "\n",
    "# Compute the residual sum of squares\n",
    "residuals = y.cpu() - torch.tensor(predictions)\n",
    "rss = torch.sum(residuals**2)\n",
    "\n",
    "# Compute R-squared\n",
    "r_squared = 1 - (rss / tss)\n",
    "\n",
    "# Number of observations\n",
    "n = y.shape[0]\n",
    "\n",
    "# Number of predictors\n",
    "p = X.shape[1]\n",
    "\n",
    "# Compute Adjusted R-squared\n",
    "adjusted_r_squared = 1 - (1 - r_squared) * ((n - 1) / (n - p - 1))\n",
    "print(f'Adjusted R-Squared: {adjusted_r_squared.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By considering the number of predictors, the Adjusted R-squared helps to avoid the risk of overfitting, which is when a model is excessively complex and includes too many parameters, leading it to perform well on the training data but poorly on unseen data. Adjusted R-Squared is therefore an essential tool for reliable and robust model selection.\n",
    "\n",
    "## Train an Adjusted Model (Multiple Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 31678758.0\n",
      "Epoch 100, Loss 14326399.0\n",
      "Epoch 200, Loss 13036275.0\n",
      "Epoch 300, Loss 11901428.0\n",
      "Epoch 400, Loss 10902685.0\n",
      "Epoch 500, Loss 10023317.0\n",
      "Epoch 600, Loss 9248715.0\n",
      "Epoch 700, Loss 8566113.0\n",
      "Epoch 800, Loss 7964342.5\n",
      "Epoch 900, Loss 7433629.5\n",
      "Epoch 1000, Loss 6965412.5\n",
      "Epoch 1100, Loss 6552189.5\n",
      "Epoch 1200, Loss 6187382.5\n",
      "Epoch 1300, Loss 5865213.5\n",
      "Epoch 1400, Loss 5580614.0\n",
      "Epoch 1500, Loss 5329129.0\n",
      "Epoch 1600, Loss 5106840.0\n",
      "Epoch 1700, Loss 4910308.0\n",
      "Epoch 1800, Loss 4736500.5\n",
      "Epoch 1900, Loss 4582750.0\n",
      "Epoch 2000, Loss 4446706.5\n",
      "Epoch 2100, Loss 4326301.0\n",
      "Epoch 2200, Loss 4219711.0\n",
      "Epoch 2300, Loss 4125325.0\n",
      "Epoch 2400, Loss 4041726.5\n",
      "Epoch 2500, Loss 3967662.5\n",
      "Epoch 2600, Loss 3902029.5\n",
      "Epoch 2700, Loss 3843851.25\n",
      "Epoch 2800, Loss 3792267.5\n",
      "Epoch 2900, Loss 3746516.75\n",
      "Epoch 3000, Loss 3705926.75\n",
      "Epoch 3100, Loss 3669904.25\n",
      "Epoch 3200, Loss 3637924.0\n",
      "Epoch 3300, Loss 3609521.0\n",
      "Epoch 3400, Loss 3584286.0\n",
      "Epoch 3500, Loss 3561854.5\n",
      "Epoch 3600, Loss 3541907.25\n",
      "Epoch 3700, Loss 3524157.75\n",
      "Epoch 3800, Loss 3508356.5\n",
      "Epoch 3900, Loss 3494279.75\n",
      "Epoch 4000, Loss 3481730.75\n",
      "Epoch 4100, Loss 3470535.5\n",
      "Epoch 4200, Loss 3460539.25\n",
      "Epoch 4300, Loss 3451606.0\n",
      "Epoch 4400, Loss 3443614.0\n",
      "Epoch 4500, Loss 3436457.0\n",
      "Epoch 4600, Loss 3430038.75\n",
      "Epoch 4700, Loss 3424276.25\n",
      "Epoch 4800, Loss 3419094.25\n",
      "Epoch 4900, Loss 3414427.25\n",
      "\n",
      "Final loss: 3410256.5\n",
      "Beta coefficients (weights): [[ 606.9656   -120.83714   -87.432076 1192.6772   1139.4635    726.2238  ]]\n",
      "Bias: -4.1131086349487305\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define the path to the diamonds.csv file\n",
    "diamonds_csv: str = './diamonds.csv'\n",
    "\n",
    "# Read the csv file into a pandas DataFrame\n",
    "diamonds: pd.DataFrame = pd.read_csv(diamonds_csv)\n",
    "\n",
    "# Select multiple features for the model\n",
    "df: pd.DataFrame = diamonds.loc[:,['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
    "\n",
    "# Extract the target variable (price) from the original dataset\n",
    "price: np.ndarray = diamonds['price'].values\n",
    "target: pd.DataFrame = pd.DataFrame(price)\n",
    "\n",
    "# Determine the device to be used for computations (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \\\n",
    "                      \"mps\" if torch.backends.mps.is_available() else \\\n",
    "                      \"cpu\")\n",
    "\n",
    "# Convert the target and features to tensors and move them to the appropriate device\n",
    "y = torch.tensor(target.values).float().to(device)\n",
    "y = y.view(y.shape[0], 1)\n",
    "X = torch.tensor(df.values).float().to(device) # Here we are using all the features ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "\n",
    "# Define the model (a multiple linear regression)\n",
    "model = torch.nn.Linear(in_features = 6, out_features = 1).to(device) # We have 6 features, so in_features is set to 6\n",
    "\n",
    "# Define the loss function (Mean Squared Error) and the optimizer (SGD)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model for 5000 epochs\n",
    "for epoch in range(5000):\n",
    "    # Forward pass: compute predicted y by passing x to the model\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss {loss.item()}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Print final loss, weights and bias\n",
    "print(f'\\nFinal loss: {loss.item()}')\n",
    "print(f'Beta coefficients (weights): {model.weight.detach().cpu().numpy()}')\n",
    "print(f'Bias: {model.bias.item()}')\n",
    "\n",
    "# Getting the model's predictions and converting them back to a numpy array\n",
    "predictions = model.cpu()(X.cpu()).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute all the metrics\n",
    "def compute_metrics(y_actual, y_pred, n, p):\n",
    "\n",
    "    # Convert the actual and predicted values to tensors\n",
    "    y_actual = torch.tensor(y_actual).float()\n",
    "    y_pred = torch.tensor(y_pred).float()\n",
    "\n",
    "    # Define the loss functions\n",
    "    loss_function_mae = torch.nn.L1Loss()\n",
    "    loss_function_mse = torch.nn.MSELoss()\n",
    "\n",
    "    # Compute Mean Absolute Error\n",
    "    mae = loss_function_mae(y_pred, y_actual)\n",
    "    print(f'Mean Absolute Error: {mae.item()}')\n",
    "\n",
    "    # Compute Mean Squared Error\n",
    "    mse = loss_function_mse(y_pred, y_actual)\n",
    "    print(f'Mean Squared Error: {mse.item()}')\n",
    "\n",
    "    # Compute Root Mean Squared Error\n",
    "    rmse = torch.sqrt(mse)\n",
    "    print(f'Root Mean Squared Error: {rmse.item()}')\n",
    "\n",
    "    # Compute the Residual Sum of Squares\n",
    "    residuals = y_actual - y_pred\n",
    "    rss = torch.sum(residuals**2)\n",
    "    print(f'Residual Sum of Squares: {rss.item()}')\n",
    "\n",
    "    # Compute the total sum of squares\n",
    "    tss = torch.sum((y_actual - torch.mean(y_actual))**2)\n",
    "\n",
    "    # Compute R-squared\n",
    "    r_squared = 1 - (rss / tss)\n",
    "    print(f'R-Squared: {r_squared.item()}')\n",
    "\n",
    "    # Compute Adjusted R-squared\n",
    "    adjusted_r_squared = 1 - (1 - r_squared) * ((n - 1) / (n - p - 1))\n",
    "    print(f'Adjusted R-Squared: {adjusted_r_squared.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1292.947021484375\n",
      "Mean Squared Error: 3410216.25\n",
      "Root Mean Squared Error: 1846.6771240234375\n",
      "Residual Sum of Squares: 183947067392.0\n",
      "R-Squared: 0.7857276201248169\n",
      "Adjusted R-Squared: 0.7857037782669067\n"
     ]
    }
   ],
   "source": [
    "# Call the function with the actual and predicted values, number of observations (n), and number of predictors (p)\n",
    "compute_metrics(y_actual = target.values,\n",
    "                y_pred = predictions,\n",
    "                n = y.shape[0],\n",
    "                p = X.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
